{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        model = models.vgg19(pretrained=True)\n",
    "        in_features = model.classifier[-1].in_features # input size \n",
    "        # print(\"Size of the features \", in_features)\n",
    "        # print(\"VGG input size \", model.classifier)\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1]) # remove output layer\n",
    "\n",
    "        self.model = model # vgg19 without output layer\n",
    "        self.fc = nn.Linear(in_features, embed_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.model(image)\n",
    "        img_feature = self.fc(img_feature)\n",
    "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
    "        img_feature = img_feature.div(l2_norm) #1xn 1d vector\n",
    "\n",
    "        # return the new encoding of the input image\n",
    "\n",
    "        return img_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape  (2, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "img_enc = ImgEncoder(1024)\n",
    "img_test = np.random.rand(2,3, 32,32) * 255\n",
    "print(\"Image shape \", img_test.shape)\n",
    "img_test = torch.tensor(img_test).float()\n",
    "\n",
    "img_feature = img_enc(img_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, featd, hidden_size, num_layers, out_size):\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, featd)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(featd, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(2*num_layers*hidden_size, out_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        qst_vec = self.embedding(question) # [batchsize, max_qst_len=30, word_emb=300]\n",
    "        # print(\"emb size \", qst_vec.shape)\n",
    "        qst_vec = self.tanh(qst_vec) # -1, 1\n",
    "        qst_vec = qst_vec.transpose(0, 1) #[max_qst_len=30, batchsize, word_emb=300]\n",
    "        _, (hidden, cell) = self.lstm(qst_vec) #[num_layer=2, batchsize, hidden_size=512]\n",
    "        qst_feature = torch.cat((hidden, cell), 2) # [num_layer=2, batchsize, 2*hiddensize=1024]\n",
    "        qst_feature = qst_feature.transpose(0, 1) #[batchsize, num_layers, hiddensize]\n",
    "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1) #[batch, featsize]\n",
    "        qst_feature = self.tanh(qst_feature)\n",
    "        qst_feature = self.fc(qst_feature)\n",
    "\n",
    "        return qst_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input size  torch.Size([1, 5])\n",
      "Shape of question feat  torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "qst = np.array([[1,2,3,4,5]])\n",
    "qst = torch.tensor(qst).int()\n",
    "qst_enc = QstEncoder(vocab_size=100, featd=128, hidden_size=512, num_layers=2, out_size=1024)\n",
    "print(\"Model input size \", qst.shape)\n",
    "qst_feat = qst_enc(qst)\n",
    "print(\"Shape of question feat \", qst_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaModel(nn.Module):\n",
    "    def __init__(self,vocab_size, feat_dim, hidden_size, num_layers, out_size ):\n",
    "        super(VqaModel, self).__init__()\n",
    "        self.img_enc = ImgEncoder(feat_dim)\n",
    "        self.qst_enc = QstEncoder(vocab_size, feat_dim, hidden_size, num_layers, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(feat_dim, out_size)\n",
    "        self.out = nn.Linear(out_size, out_size)\n",
    "        self.outsoft = nn.Softmax()\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        img_feat = self.img_enc(img)\n",
    "        qst_feat = self.qst_enc(qst)\n",
    "        combined_feat = torch.mul(img_feat, qst_feat)\n",
    "        combined_feat = self.tanh(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        combined_feat = self.fc1(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        output_probs = self.out(combined_feat) #[batch_size, vocab_size]\n",
    "        softmaxout = self.outsoft(output_probs)\n",
    "        return output_probs, softmaxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True)\n",
    "\n",
    "# Set the recursion limit to a higher value temporarily\n",
    "sys.setrecursionlimit(50000)\n",
    "# xtrain = np.load(\"COCO-2015/datasets/train_drive_April_27.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_loader\n",
    "data_loader = get_loader(\n",
    "    input_dir='./COCO-2015/datasets',\n",
    "    input_vqa_train='train.npy',\n",
    "    input_vqa_valid='valid.npy',\n",
    "    max_qst_length=30,\n",
    "    max_num_ans=10,\n",
    "    batch_size=16,\n",
    "    num_workers=1)\n",
    "embed_size = 1204\n",
    "word_embed_size = 300\n",
    "num_layers = 2\n",
    "hidden_size = 300\n",
    "qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n",
    "ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size Qst 17856\n",
      "Vocab size Ans 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size Qst {qst_vocab_size}')\n",
    "print(f'Vocab size Ans {ans_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qamodel = VqaModel(vocab_size=qst_vocab_size, feat_dim=word_embed_size, hidden_size=hidden_size, num_layers=num_layers, out_size=ans_vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qamodel = torch.load('models/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(qamodel.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "early_stop_threshold = 3\n",
    "best_loss = 99999\n",
    "val_increase_count = 0\n",
    "stop_training = False\n",
    "prev_loss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| VALID SET | Epoch [01/50], Loss: 2.5864 3397], Loss: 1.8578, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [02/50], Loss: 2.3704 3397], Loss: 1.9585, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [03/50], Loss: 2.2695 3397], Loss: 1.4388, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [04/50], Loss: 2.2222 3397], Loss: 2.3398, Acc: 56.2500\n",
      "\n",
      "| VALID SET | Epoch [05/50], Loss: 2.1989 3397], Loss: 2.5075, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [06/50], Loss: 2.1717 3397], Loss: 2.2345, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [07/50], Loss: 2.1597 3397], Loss: 2.6038, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [08/50], Loss: 2.1431 3397], Loss: 2.3243, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [09/50], Loss: 2.1448 3397], Loss: 2.2134, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [10/50], Loss: 2.0479 3397], Loss: 1.3444, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [11/50], Loss: 2.0347 3397], Loss: 2.1239, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [12/50], Loss: 2.0304 3397], Loss: 2.0790, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [13/50], Loss: 2.0262 3397], Loss: 2.1723, Acc: 56.2500\n",
      "\n",
      "| VALID SET | Epoch [14/50], Loss: 2.0250 3397], Loss: 1.1808, Acc: 75.0000\n",
      "\n",
      "| VALID SET | Epoch [15/50], Loss: 2.0220 3397], Loss: 1.7666, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [16/50], Loss: 2.0199 3397], Loss: 1.5079, Acc: 62.5000\n",
      "\n",
      "| VALID SET | Epoch [17/50], Loss: 2.0234 3397], Loss: 1.8183, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [18/50], Loss: 2.0204 3397], Loss: 2.4794, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [19/50], Loss: 2.0194 3397], Loss: 2.2725, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [20/50], Loss: 2.0177 3397], Loss: 2.1072, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [21/50], Loss: 2.0169 3397], Loss: 1.6220, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [22/50], Loss: 2.0138 3397], Loss: 2.1900, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [23/50], Loss: 2.0172 3397], Loss: 1.7453, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [24/50], Loss: 2.0173 3397], Loss: 1.9229, Acc: 56.2500\n",
      "\n",
      "| VALID SET | Epoch [25/50], Loss: 2.0151 3397], Loss: 2.0108, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [26/50], Loss: 2.0137 3397], Loss: 2.7188, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [27/50], Loss: 2.0136 3397], Loss: 1.6100, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [28/50], Loss: 2.0192 3397], Loss: 2.1989, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [29/50], Loss: 2.0199 3397], Loss: 2.0840, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [30/50], Loss: 2.0179 3397], Loss: 1.4244, Acc: 62.5000\n",
      "\n",
      "| VALID SET | Epoch [31/50], Loss: 2.0174 3397], Loss: 3.3186, Acc: 18.7500\n",
      "\n",
      "| VALID SET | Epoch [32/50], Loss: 2.0167 3397], Loss: 1.3817, Acc: 62.5000\n",
      "\n",
      "| VALID SET | Epoch [33/50], Loss: 2.0163 3397], Loss: 3.0989, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [34/50], Loss: 2.0164 3397], Loss: 1.3416, Acc: 56.2500\n",
      "\n",
      "| VALID SET | Epoch [35/50], Loss: 2.0152 3397], Loss: 2.0333, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [36/50], Loss: 2.0143 3397], Loss: 1.4751, Acc: 62.5000\n",
      "\n",
      "| VALID SET | Epoch [37/50], Loss: 2.0162 3397], Loss: 1.3313, Acc: 56.2500\n",
      "\n",
      "| VALID SET | Epoch [38/50], Loss: 2.0112 3397], Loss: 3.1666, Acc: 25.0000\n",
      "\n",
      "| VALID SET | Epoch [39/50], Loss: 2.0127 3397], Loss: 2.2174, Acc: 56.2500\n",
      "\n",
      "| VALID SET | Epoch [40/50], Loss: 2.0188 3397], Loss: 2.1345, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [41/50], Loss: 2.0162 3397], Loss: 1.5980, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [42/50], Loss: 2.0150 3397], Loss: 1.9556, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [43/50], Loss: 2.0124 3397], Loss: 1.5721, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [44/50], Loss: 2.0129 3397], Loss: 2.0755, Acc: 37.5000\n",
      "\n",
      "| VALID SET | Epoch [45/50], Loss: 2.0160 3397], Loss: 1.6911, Acc: 50.0000\n",
      "\n",
      "| VALID SET | Epoch [46/50], Loss: 2.0119 3397], Loss: 1.5365, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [47/50], Loss: 2.0147 3397], Loss: 0.8259, Acc: 62.5000\n",
      "\n",
      "| VALID SET | Epoch [48/50], Loss: 2.0152 3397], Loss: 2.1328, Acc: 31.2500\n",
      "\n",
      "| VALID SET | Epoch [49/50], Loss: 2.0135 3397], Loss: 1.4823, Acc: 43.7500\n",
      "\n",
      "| VALID SET | Epoch [50/50], Loss: 2.0122 3397], Loss: 1.9252, Acc: 43.7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 50\n",
    "batch_size = 16\n",
    "running_loss = 0\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    for phase in ['train', 'valid']:\n",
    "        running_loss, running_corr_exp1, running_corr_exp2 = 0., 0, 0\n",
    "        batch_step_size = len(data_loader[phase].dataset)/batch_size\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            qamodel.train()\n",
    "        else:\n",
    "            qamodel.eval()\n",
    "        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
    "            image = batch_sample['image'].to(device) # 128 x 64 x 64 x 3\n",
    "            question = batch_sample['question'].to(device) # 128 x 30\n",
    "            label = batch_sample['answer_label'].to(device)\n",
    "            multi_choice = batch_sample['answer_multi_choice']\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                output, soft_probs = qamodel(image, question) # [batch_size, ans_vocabsize=1000]\n",
    "                loss = criterion(output, label)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total = len(label)\n",
    "            correct = (predicted == label).sum().item()\n",
    "            acc = 100 * (correct/total)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Acc: {:.4f}'\n",
    "                        .format(phase.upper(), epoch+1, TOTAL_EPOCHS, batch_idx, int(batch_step_size), loss.item(), acc), end = '\\r')                \n",
    "    epoch_loss = running_loss / batch_step_size\n",
    "\n",
    "    print('| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f} \\n'.format(phase.upper(), epoch+1, TOTAL_EPOCHS, epoch_loss))\n",
    "\n",
    "    # with open(os.path.join('logs', '{}-{}-log-epoch-{:02}.txt')\n",
    "    #             .format('./models/', phase, epoch+1), 'w') as f:\n",
    "    #     f.write(str(epoch+1) + '\\t'\n",
    "    #             + str(epoch_loss) + '\\t')\n",
    "\n",
    "\n",
    "    if phase == 'valid':\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(qamodel.state_dict(), os.path.join('modelsv2', 'best_coco_base_model.pt'))\n",
    "        if epoch_loss > prev_loss:\n",
    "            val_increase_count += 1\n",
    "        else:\n",
    "            val_increase_count = 0\n",
    "        if val_increase_count >= early_stop_threshold:\n",
    "            stop_training = True\n",
    "        prev_loss = epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6b0e9ae300e93b4feb3afd8142be7999ae65ede1dda2441330c407f00ece528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
