{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the notebook:\n",
    "In this code notebook, we provide implementation of generating explanation on an autonomous vehicle's action using a visual question answering (VQA) approach. Briefly, we fine-tune the pretrained VGG-19 architecture on the video data provided by the DDPG-based autonomous driving on the CARLA simulator. We then combine the obtained image features with question encoder acquired by LSTM. The image and question encoder is then passed to a fully-connected layer and softmax probability is applied. By this way, we select top 5 probability scores with corresponding explanations out of possible 1000 explanatory answer vocabulary and the explanation with the highest probability score becomes an answer to the asked question about the action performed within that scene. For instance, below we show an autonomous car going straight in the image frame. If we ask \"Why was moving forward decided?\", the softmax produces top 5 explanations and the explanation with the highest score becomes an answer to this question on the performed action of a car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The activations from the last hidden layer of VGG-19 with L2 normalization as 4096-dim image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        model = models.vgg19(pretrained=True)\n",
    "        in_features = model.classifier[-1].in_features # input size \n",
    "        # print(\"Size of the features \", in_features)\n",
    "        # print(\"VGG input size \", model.classifier)\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1]) # remove output layer\n",
    "\n",
    "        self.model = model # vgg19 without output layer\n",
    "        self.fc = nn.Linear(in_features, embed_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.model(image)\n",
    "        img_feature = self.fc(img_feature)\n",
    "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
    "        img_feature = img_feature.div(l2_norm) #1xn 1d vector\n",
    "\n",
    "        # return the new encoding of the input image\n",
    "\n",
    "        return img_feature\n",
    "    \n",
    "class QstEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, featd, hidden_size, num_layers, out_size):\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, featd)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(featd, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(2*num_layers*hidden_size, out_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        qst_vec = self.embedding(question) # [batchsize, max_qst_len=30, word_emb=300]\n",
    "        # print(\"emb size \", qst_vec.shape)\n",
    "        qst_vec = self.tanh(qst_vec) # -1, 1\n",
    "        qst_vec = qst_vec.transpose(0, 1) #[max_qst_len=30, batchsize, word_emb=300]\n",
    "        _, (hidden, cell) = self.lstm(qst_vec) #[num_layer=2, batchsize, hidden_size=512]\n",
    "        qst_feature = torch.cat((hidden, cell), 2) # [num_layer=2, batchsize, 2*hiddensize=1024]\n",
    "        qst_feature = qst_feature.transpose(0, 1) #[batchsize, num_layers, hiddensize]\n",
    "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1) #[batch, featsize]\n",
    "        qst_feature = self.tanh(qst_feature)\n",
    "        qst_feature = self.fc(qst_feature)\n",
    "\n",
    "        return qst_feature\n",
    "\n",
    "class VqaModel(nn.Module):\n",
    "    def __init__(self,vocab_size, feat_dim, hidden_size, num_layers, out_size ):\n",
    "        super(VqaModel, self).__init__()\n",
    "        self.img_enc = ImgEncoder(feat_dim)\n",
    "        self.qst_enc = QstEncoder(vocab_size, feat_dim, hidden_size, num_layers, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(feat_dim, out_size)\n",
    "        self.out = nn.Linear(out_size, out_size)\n",
    "        self.outsoft = nn.Softmax()\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        img_feat = self.img_enc(img)\n",
    "        qst_feat = self.qst_enc(qst)\n",
    "        combined_feat = torch.mul(img_feat, qst_feat)\n",
    "        combined_feat = self.tanh(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        combined_feat = self.fc1(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        output_probs = self.out(combined_feat) #[batch_size, vocab_size]\n",
    "        softmaxout = self.outsoft(output_probs)\n",
    "        return output_probs, softmaxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "qst_vocab_size = 17856\n",
    "ans_vocab_size = 1000\n",
    "\n",
    "embed_size = 1204\n",
    "word_embed_size = 300\n",
    "num_layers = 2\n",
    "hidden_size = 300\n",
    "qamodel = VqaModel(vocab_size=qst_vocab_size, feat_dim=word_embed_size, hidden_size=hidden_size, num_layers=num_layers, out_size=ans_vocab_size).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the fine-tuned driving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qamodel.load_state_dict(torch.load('modelsv2/best_drive_model_apr28.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VqaModel(\n",
       "  (img_enc): ImgEncoder(\n",
       "    (model): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (17): ReLU(inplace=True)\n",
       "        (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (24): ReLU(inplace=True)\n",
       "        (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (26): ReLU(inplace=True)\n",
       "        (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (31): ReLU(inplace=True)\n",
       "        (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (33): ReLU(inplace=True)\n",
       "        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (35): ReLU(inplace=True)\n",
       "        (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=4096, out_features=300, bias=True)\n",
       "  )\n",
       "  (qst_enc): QstEncoder(\n",
       "    (embedding): Embedding(17856, 300)\n",
       "    (tanh): Tanh()\n",
       "    (lstm): LSTM(300, 300, num_layers=2)\n",
       "    (fc): Linear(in_features=1200, out_features=300, bias=True)\n",
       "  )\n",
       "  (tanh): Tanh()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=300, out_features=1000, bias=True)\n",
       "  (out): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (outsoft): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qamodel = qamodel.to(device)\n",
    "qamodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_str_list(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "qst_vocab = load_str_list(\"./COCO-2015/datasets/vocab_questions.txt\")\n",
    "ans_vocab = load_str_list(\"./COCO-2015/datasets/vocab_answers_VQA.txt\")\n",
    "word_to_index_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n",
    "unknown_to_index = word_to_index_dict['<unk>'] if '<unk>' in word_to_index_dict else None\n",
    "vocab_size = len(qst_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(w):\n",
    "    if w in word_to_index_dict:\n",
    "        return word_to_index_dict[w]\n",
    "    elif unknown_to_index is not None:\n",
    "         return unknown_to_index\n",
    " \n",
    "    else:\n",
    "        raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on a sample image to see the top probable explanations (i.e., answer) to a question on a corresponding driving scene action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_qst_length=30\n",
    "\n",
    "question = 'Why is the car turning to the right?'\n",
    "question=question.lower()\n",
    "q_list = list(question.split(\" \"))\n",
    "#     print(q_list)\n",
    "\n",
    "idx = 'valid'\n",
    "qst2idc = np.array([word_to_index('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
    "qst2idc[:len(q_list)] = [word_to_index(w) for w in q_list]\n",
    "\n",
    "question = qst2idc\n",
    "question = torch.from_numpy(question).long()\n",
    "\n",
    "question = question.to(device)\n",
    "question = question.unsqueeze(dim=0)\n",
    "import cv2\n",
    "image = cv2.imread(\"./Examples_for_paper_folder/1. turn_right_seg2_Town2_driving_frame7900_Top_column.jpg\")\n",
    "image = cv2.resize(image, (640, 480)) \n",
    "image = torch.from_numpy(image).float()\n",
    "image = image.to(device)\n",
    "image = image.unsqueeze(dim=0)\n",
    "image = image.view(1,3,640,480)\n",
    "output, probs = qamodel(image, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, indices = torch.topk(probs, k=5, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top five predictions with the corresponding probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions with the probability scores:\n",
      "'Because the road is bending to the right.' - 0.844\n",
      "'Because the road is bending to the left.' - 0.053\n",
      "'orange' - 0.013\n",
      "'Because the road is clear.' - 0.009\n",
      "'can' - 0.009\n"
     ]
    }
   ],
   "source": [
    "probs = probs.squeeze()\n",
    "indices = indices.squeeze()\n",
    "print(\"Top 5 predictions with the probability scores:\")\n",
    "for i in range(5):\n",
    "    print(\"'{}' - {:.3f}\".format(ans_vocab[indices[i].item()], probs[i].item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation our of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "all_test_images = []\n",
    "image_folder=\"./Selected_segments_frames_test_data/\"\n",
    "for filename in os.listdir(image_folder):\n",
    "        img = cv2.imread(os.path.join(image_folder,filename))\n",
    "        if img is not None:\n",
    "            all_test_images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_set=pd.read_csv('./annotations/Autonomous_Driving_Question_Answering_Annotation_Testing - REVISED_April_27.csv')\n",
    "test_set_questions=test_set['Question'].tolist()\n",
    "test_set_answers=test_set['Answer'].tolist()\n",
    "test_imgs_path=test_set['Path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for top prediction  0.8\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for ims,qus, ground_truth in zip(test_imgs_path, test_set_questions, test_set_answers):\n",
    "    img = cv2.imread(ims)\n",
    "    img = cv2.resize(img, (640, 480))  # making sure that the images are in the standard size of 640 x 480.\n",
    "    img = torch.from_numpy(img).float()\n",
    "    img = img.to(device)\n",
    "    img = img.unsqueeze(dim=0)\n",
    "    img = img.view(1,3,640,480)\n",
    "\n",
    "    q_list = list(qus.lower().split(\" \"))\n",
    "    #     print(q_list)\n",
    "\n",
    "    idx = 'valid'\n",
    "    qst2idc = np.array([word_to_index('<pad>')] * max_qst_length)  \n",
    "    qst2idc[:len(q_list)] = [word_to_index(w) for w in q_list]\n",
    "\n",
    "    question = qst2idc\n",
    "    question = torch.from_numpy(question).long()\n",
    "\n",
    "    question = question.to(device)\n",
    "    question = question.unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output, probs = qamodel(img, question)\n",
    "    probs, indices = torch.topk(probs, k=5, dim=1)\n",
    "    probs = probs.squeeze()\n",
    "    indices = indices.squeeze()\n",
    "    prediction_list=[]\n",
    "    score_list=[]\n",
    "    \n",
    "    for i in range(5):\n",
    "        prediction_list.append(ans_vocab[indices[i].item()])\n",
    "        score_list.append(probs[i].item())\n",
    "        #print(ground_truth) \n",
    "        #print(prediction_list)\n",
    "        #formatted_scores = ['{:.4f}'.format(score) for score in score_list]\n",
    "        #print(', '.join(formatted_scores))\n",
    "       \n",
    "        break\n",
    "    if prediction_list[0].lower().strip() == ground_truth.lower().strip():\n",
    "        cnt += 1\n",
    "           \n",
    "print (\"Model accuracy for top prediction \", (cnt/len(test_set_answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6b0e9ae300e93b4feb3afd8142be7999ae65ede1dda2441330c407f00ece528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
