{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the notebook:\n",
    "In this notebook, we provide implementation of generating explanation on an autonomous vehicle's action using a visual question answering (VQA) approach. Briefly, we fine-tune the pretrained VGG-19 architecture on the video data provided by the DDPG-based autonomous driving on the CARLA simulator. We then elementwise multiply the obtained image features with question encoder acquired by LSTM. The resultant vector is then passed to a fully-connected layer and softmax probability is applied. By this way, we select top 5 probability scores with corresponding explanations out of possible 1000 explanatory answer vocabulary and the explanation with the highest probability score becomes an answer to the asked question about the action performed within that scene. For instance, below we show an autonomous car going straight in the image frame. If we ask \"Why is going straight decided?\", the softmax produces top 5 explanations and the explanation with the highest score (i.e., Because road is clear.) becomes an answer to this question on the performed action of a car at that scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The activations from the last hidden layer of VGG-19 with L2 normalization as 4096-dim image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        model = models.vgg19(pretrained=True)\n",
    "        in_features = model.classifier[-1].in_features # input size \n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1]) # remove output layer\n",
    "\n",
    "        self.model = model # vgg19 without output layer\n",
    "        self.fc = nn.Linear(in_features, embed_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.model(image)\n",
    "        img_feature = self.fc(img_feature)\n",
    "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
    "        img_feature = img_feature.div(l2_norm) #1xn 1d vector\n",
    "\n",
    "        # return the new encoding of the input image\n",
    "\n",
    "        return img_feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM to get 1024-dim embedding as the question encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, featd, hidden_size, num_layers, out_size):\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, featd)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(featd, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(2*num_layers*hidden_size, out_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        qst_vec = self.embedding(question) # [batchsize, max_qst_len=30, word_emb=300]\n",
    "        \n",
    "        qst_vec = self.tanh(qst_vec) \n",
    "        qst_vec = qst_vec.transpose(0, 1) \n",
    "        _, (hidden, cell) = self.lstm(qst_vec)\n",
    "        qst_feature = torch.cat((hidden, cell), 2) # [num_layer=2, batchsize, 2*hiddensize=1024]\n",
    "        qst_feature = qst_feature.transpose(0, 1) \n",
    "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1) \n",
    "        qst_feature = self.tanh(qst_feature)\n",
    "        qst_feature = self.fc(qst_feature)\n",
    "\n",
    "        return qst_feature\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining image encoder and question embedding as elementwise multiplication, passing to the fully connected layer, and applying softmax probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaModel(nn.Module):\n",
    "    def __init__(self,vocab_size, feat_dim, hidden_size, num_layers, out_size ):\n",
    "        super(VqaModel, self).__init__()\n",
    "        self.img_enc = ImgEncoder(out_size)\n",
    "        self.qst_enc = QstEncoder(vocab_size, feat_dim, hidden_size, num_layers, out_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(out_size, vocab_size)\n",
    "        self.out = nn.Linear(vocab_size, vocab_size)\n",
    "        self.outsoft = nn.Softmax()\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        img_feat = self.img_enc(img)\n",
    "        qst_feat = self.qst_enc(qst)\n",
    "        combined_feat = torch.mul(img_feat, qst_feat)\n",
    "        combined_feat = self.tanh(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        combined_feat = self.fc1(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        output_probs = self.out(combined_feat) #[batch_size, vocab_size]\n",
    "        softmaxout = self.outsoft(output_probs)\n",
    "        return output_probs, softmaxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 1204\n",
    "word_embed_size = 300\n",
    "num_layers = 2\n",
    "hidden_size = 512\n",
    "qst_vocab_size = 17856\n",
    "ans_vocab_size = 1000\n",
    "\n",
    "qamodel = VqaModel(vocab_size=qst_vocab_size, feat_dim=word_embed_size, hidden_size=hidden_size, num_layers=num_layers, out_size=ans_vocab_size).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the fine-tuned driving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qamodel.load_state_dict(torch.load('modelsv2/best_model_drive.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VqaModel(\n",
       "  (img_enc): ImgEncoder(\n",
       "    (model): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (17): ReLU(inplace=True)\n",
       "        (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (24): ReLU(inplace=True)\n",
       "        (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (26): ReLU(inplace=True)\n",
       "        (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (31): ReLU(inplace=True)\n",
       "        (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (33): ReLU(inplace=True)\n",
       "        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (35): ReLU(inplace=True)\n",
       "        (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       "  (qst_enc): QstEncoder(\n",
       "    (embedding): Embedding(17856, 300)\n",
       "    (tanh): Tanh()\n",
       "    (lstm): LSTM(300, 512, num_layers=2)\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (tanh): Tanh()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1000, out_features=17856, bias=True)\n",
       "  (out): Linear(in_features=17856, out_features=17856, bias=True)\n",
       "  (outsoft): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qamodel = qamodel.to(device)\n",
    "qamodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_str_list(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "qst_vocab = load_str_list(\"./COCO-2015/datasets/vocab_questions.txt\")\n",
    "ans_vocab = load_str_list(\"./COCO-2015/datasets/vocab_answers.txt\")\n",
    "word_to_index_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n",
    "unknown_to_index = word_to_index_dict['<unk>'] if '<unk>' in word_to_index_dict else None\n",
    "vocab_size = len(qst_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(w):\n",
    "    if w in word_to_index_dict:\n",
    "        return word_to_index_dict[w]\n",
    "    elif unknown_to_index is not None:\n",
    "         return unknown_to_index\n",
    " \n",
    "    else:\n",
    "        raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on a sample image to see the top probable explanations (i.e., answer) to a question on a corresponding driving scene action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_qst_length=30\n",
    "\n",
    "question = 'why is going straight decided?'\n",
    "q_list = list(question.split(\" \"))\n",
    "#     print(q_list)\n",
    "\n",
    "idx = 'valid'\n",
    "qst2idc = np.array([word_to_index('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
    "qst2idc[:len(q_list)] = [word_to_index(w) for w in q_list]\n",
    "\n",
    "question = qst2idc\n",
    "question = torch.from_numpy(question).long()\n",
    "\n",
    "question = question.to(device)\n",
    "question = question.unsqueeze(dim=0)\n",
    "import cv2\n",
    "image = cv2.imread(\"./Selected_segments_frames_test_data/img_test.png\")\n",
    "image = cv2.resize(image, (640, 480)) \n",
    "image = torch.from_numpy(image).float()\n",
    "image = image.to(device)\n",
    "image = image.unsqueeze(dim=0)\n",
    "image = image.view(1,3,640,480)\n",
    "output, probs = qamodel(image, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, indices = torch.topk(probs, k=5, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model picks up the correct  answer with ~0.97 probability score on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions with the probability scores:\n",
      "'Because road is clear.' - 0.9726\n",
      "'Because the road is bending to the left.' - 0.0176\n",
      "'<unk>' - 0.0082\n",
      "'Because the road is bending to the right.' - 0.0011\n",
      "'can' - 0.0001\n"
     ]
    }
   ],
   "source": [
    "probs = probs.squeeze()\n",
    "indices = indices.squeeze()\n",
    "print(\"Top 5 predictions with the probability scores:\")\n",
    "for i in range(5):\n",
    "    print(\"'{}' - {:.4f}\".format(ans_vocab[indices[i].item()], probs[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6b0e9ae300e93b4feb3afd8142be7999ae65ede1dda2441330c407f00ece528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
