{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        model = models.vgg19(pretrained=True)\n",
    "        in_features = model.classifier[-1].in_features # input size \n",
    "        # print(\"Size of the features \", in_features)\n",
    "        # print(\"VGG input size \", model.classifier)\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1]) # remove output layer\n",
    "\n",
    "        self.model = model # vgg19 without output layer\n",
    "        self.fc = nn.Linear(in_features, embed_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.model(image)\n",
    "        img_feature = self.fc(img_feature)\n",
    "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
    "        img_feature = img_feature.div(l2_norm) #1xn 1d vector\n",
    "\n",
    "        # return the new encoding of the input image\n",
    "\n",
    "        return img_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, featd, hidden_size, num_layers, out_size):\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, featd)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(featd, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(2*num_layers*hidden_size, out_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        qst_vec = self.embedding(question) # [batchsize, max_qst_len=30, word_emb=300]\n",
    "        # print(\"emb size \", qst_vec.shape)\n",
    "        qst_vec = self.tanh(qst_vec) # -1, 1\n",
    "        qst_vec = qst_vec.transpose(0, 1) #[max_qst_len=30, batchsize, word_emb=300]\n",
    "        _, (hidden, cell) = self.lstm(qst_vec) #[num_layer=2, batchsize, hidden_size=512]\n",
    "        qst_feature = torch.cat((hidden, cell), 2) # [num_layer=2, batchsize, 2*hiddensize=1024]\n",
    "        qst_feature = qst_feature.transpose(0, 1) #[batchsize, num_layers, hiddensize]\n",
    "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1) #[batch, featsize]\n",
    "        qst_feature = self.tanh(qst_feature)\n",
    "        qst_feature = self.fc(qst_feature)\n",
    "\n",
    "        return qst_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaModel(nn.Module):\n",
    "    def __init__(self,vocab_size, feat_dim, hidden_size, num_layers, out_size ):\n",
    "        super(VqaModel, self).__init__()\n",
    "        self.img_enc = ImgEncoder(feat_dim)\n",
    "        self.qst_enc = QstEncoder(vocab_size, feat_dim, hidden_size, num_layers, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(feat_dim, out_size)\n",
    "        self.out = nn.Linear(out_size, out_size)\n",
    "        self.outsoft = nn.Softmax()\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        img_feat = self.img_enc(img)\n",
    "        qst_feat = self.qst_enc(qst)\n",
    "        combined_feat = torch.mul(img_feat, qst_feat)\n",
    "        combined_feat = self.tanh(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        combined_feat = self.fc1(combined_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "        output_probs = self.out(combined_feat) #[batch_size, vocab_size]\n",
    "        softmaxout = self.outsoft(output_probs)\n",
    "        return output_probs, softmaxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "qst_vocab_size = 17856\n",
    "ans_vocab_size = 1000\n",
    "\n",
    "embed_size = 1204\n",
    "word_embed_size = 300\n",
    "num_layers = 2\n",
    "hidden_size = 300\n",
    "qamodel = VqaModel(vocab_size=qst_vocab_size, feat_dim=word_embed_size, hidden_size=hidden_size, num_layers=num_layers, out_size=ans_vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qamodel.load_state_dict(torch.load('modelsv2/best_coco_base_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_loader\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True)\n",
    "\n",
    "# Set the recursion limit to a higher value temporarily\n",
    "sys.setrecursionlimit(50000)\n",
    "# xtrain = np.load(\"COCO-2015/datasets/train_drive_April_27.npy\")\n",
    "data_loader = get_loader(\n",
    "    input_dir='./COCO-2015/datasets',\n",
    "    input_vqa_train='train_drive_April_28.npy',\n",
    "    input_vqa_valid='train_drive_April_28.npy',\n",
    "    max_qst_length=30,\n",
    "    max_num_ans=10,\n",
    "    batch_size=16,\n",
    "    num_workers=1)\n",
    "embed_size = 1204\n",
    "word_embed_size = 300\n",
    "num_layers = 2\n",
    "hidden_size = 300\n",
    "qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n",
    "ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size Qst 17856\n",
      "Vocab size Ans 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size Qst {qst_vocab_size}')\n",
    "print(f'Vocab size Ans {ans_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(qamodel.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "early_stop_threshold = 3\n",
    "best_loss = 99999\n",
    "val_increase_count = 0\n",
    "stop_training = False\n",
    "prev_loss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/shahin/anaconda3/envs/videoqa/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| VALID SET | Epoch [01/10], Loss: 1.1640 15], Loss: 1.2065, Acc: 87.5000\n",
      "\n",
      "| VALID SET | Epoch [02/10], Loss: 0.3429 15], Loss: 0.4085, Acc: 87.5000\n",
      "\n",
      "| VALID SET | Epoch [03/10], Loss: 0.1894 15], Loss: 0.1942, Acc: 93.7500\n",
      "\n",
      "| VALID SET | Epoch [04/10], Loss: 0.1699 15], Loss: 0.1761, Acc: 87.50000\n",
      "\n",
      "| VALID SET | Epoch [05/10], Loss: 0.1358 15], Loss: 0.0755, Acc: 93.7500\n",
      "\n",
      "| VALID SET | Epoch [06/10], Loss: 0.0812 15], Loss: 0.0798, Acc: 100.0000\n",
      "\n",
      "| VALID SET | Epoch [07/10], Loss: 0.0621 15], Loss: 0.0595, Acc: 100.0000\n",
      "\n",
      "| VALID SET | Epoch [08/10], Loss: 0.0452 15], Loss: 0.0425, Acc: 100.0000\n",
      "\n",
      "| VALID SET | Epoch [09/10], Loss: 0.0359 15], Loss: 0.0214, Acc: 100.0000\n",
      "\n",
      "| VALID SET | Epoch [10/10], Loss: 0.0372 15], Loss: 0.0166, Acc: 100.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "TOTAL_EPOCHS = 10\n",
    "batch_size = 16\n",
    "running_loss = 0\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    for phase in ['train', 'valid']:\n",
    "        running_loss, running_corr_exp1, running_corr_exp2 = 0., 0, 0\n",
    "        batch_step_size = len(data_loader[phase].dataset)/batch_size\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            qamodel.train()\n",
    "        else:\n",
    "            qamodel.eval()\n",
    "        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
    "            image = batch_sample['image'].to(device) # 128 x 64 x 64 x 3\n",
    "            question = batch_sample['question'].to(device) # 128 x 30\n",
    "            label = batch_sample['answer_label'].to(device)\n",
    "            multi_choice = batch_sample['answer_multi_choice']\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                output, soft_probs = qamodel(image, question) # [batch_size, ans_vocabsize=1000]\n",
    "                loss = criterion(output, label)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total = len(label)\n",
    "            correct = (predicted == label).sum().item()\n",
    "            acc = 100 * (correct/total)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Acc: {:.4f}'\n",
    "                        .format(phase.upper(), epoch+1, TOTAL_EPOCHS, batch_idx, int(batch_step_size), loss.item(), acc), end = '\\r')                \n",
    "    epoch_loss = running_loss / batch_step_size\n",
    "\n",
    "    print('| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f} \\n'.format(phase.upper(), epoch+1, TOTAL_EPOCHS, epoch_loss))\n",
    "\n",
    "    # with open(os.path.join('logs', '{}-{}-log-epoch-{:02}.txt')\n",
    "    #             .format('./models/', phase, epoch+1), 'w') as f:\n",
    "    #     f.write(str(epoch+1) + '\\t'\n",
    "    #             + str(epoch_loss) + '\\t')\n",
    "\n",
    "\n",
    "    if phase == 'valid':\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(qamodel.state_dict(), os.path.join('modelsv2', 'best_drive_model_apr28.pt'))\n",
    "        if epoch_loss > prev_loss:\n",
    "            val_increase_count += 1\n",
    "        else:\n",
    "            val_increase_count = 0\n",
    "        if val_increase_count >= early_stop_threshold:\n",
    "            stop_training = True\n",
    "        prev_loss = epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6b0e9ae300e93b4feb3afd8142be7999ae65ede1dda2441330c407f00ece528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
